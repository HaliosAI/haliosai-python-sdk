#!/usr/bin/env python3
"""
HaliosAI SDK - Example 5: Performance# Pattern 3: Fast Stre# Pattern 4: Balanced Streaming (balanced performance/safety)
@guarded_chat_completion(
    agent_id=HALIOS_AGENT_ID,
    streaming_guardrails=True,
    stream_buffer_size=100,
    stream_check_interval=1.0,
    guardrail_timeout=3.0
)ow latency, frequent checks)
@guarded_chat_completion(
    agent_id=HALIOS_AGENT_ID,
    streaming_guardrails=True,
    stream_buffer_size=30,
    stream_check_interval=0.2,
    guardrail_timeout=1.0
)s and Optimization

This example demonstrates different usage patterns and their performance characteristics.
Compare sequential vs concurrent processing, and learn optimization techniques.

Requirements:
    pip install haliosai
    pip install openai

Environment Variables:
    HALIOS_API_KEY: Your HaliosAI API key
    HALIOS_AGENT_ID: Your agent ID  
    OPENAI_API_KEY: Your OpenAI API key (optional, for real testing)
"""

import asyncio
import os
import time
import statistics
from typing import List, Dict, Any
from openai import AsyncOpenAI
from haliosai import guarded_chat_completion, HaliosGuard, ParallelGuardedChat

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "demo-key")
HALIOS_AGENT_ID = os.getenv("HALIOS_AGENT_ID", "demo-agent-performance")

# Mock LLM for consistent timing tests
async def mock_llm_call(messages: List[Dict[str, Any]], delay: float = 0.5) -> Dict[str, Any]:
    """Mock LLM call with configurable delay"""
    await asyncio.sleep(delay)
    return {
        "choices": [{
            "message": {
                "role": "assistant",
                "content": f"Mock response to: {messages[-1].get('content', 'No content')[:50]}..."
            }
        }],
        "usage": {"total_tokens": 100}
    }

# Pattern 1: Sequential Processing (debugging/development)
@guarded_chat_completion(
    agent_id=HALIOS_AGENT_ID,
    concurrent_guardrail_processing=False,
    guardrail_timeout=2.0
)
async def sequential_llm_call(messages: List[Dict[str, Any]]):
    """Sequential: Guardrails ‚Üí LLM ‚Üí Guardrails (easier to debug)"""
    return await mock_llm_call(messages, delay=0.3)

# Pattern 2: Concurrent Processing (production recommended)
# Pattern 2: Concurrent Processing (recommended for production)
@guarded_chat_completion(
    agent_id=HALIOS_AGENT_ID,
    concurrent_guardrail_processing=True,
    guardrail_timeout=2.0
)
async def concurrent_llm_call(messages: List[Dict[str, Any]]):
    """Concurrent: Guardrails || LLM (optimal performance)"""
    return await mock_llm_call(messages, delay=0.3)

# Pattern 3: Fast Streaming (real-time applications)
@guarded_chat_completion(
    agent_id=HALIOS_AGENT_ID,
    streaming_guardrails=True,
    stream_buffer_size=30,
    stream_check_interval=0.2,
    guardrail_timeout=1.0
)
async def fast_streaming_call(messages: List[Dict[str, Any]]):
    """Fast streaming with frequent guardrail checks"""
    chunks = ["Hello ", "from ", "HaliosAI! ", "This ", "is ", "a ", "streaming ", "response."]
    for chunk in chunks:
        await asyncio.sleep(0.05)
        yield {"choices": [{"delta": {"content": chunk}}]}

# Pattern 4: Balanced Streaming (general purpose)
@guarded_chat_completion(
    agent_id=HALIOS_AGENT_ID,
    streaming_guardrails=True,
    stream_buffer_size=100,
    stream_check_interval=1.0,
    guardrail_timeout=3.0
)
async def balanced_streaming_call(messages: List[Dict[str, Any]]):
    """Balanced streaming with moderate guardrail checks"""
    chunks = ["This is a balanced streaming response ", "with moderate checking frequency. ",
             "It provides good performance while maintaining safety. "]
    for chunk in chunks:
        await asyncio.sleep(0.2)
        yield {"choices": [{"delta": {"content": chunk}}]}

async def benchmark_processing_patterns():
    """Compare sequential vs concurrent processing performance"""
    print("‚ö° Processing Pattern Performance Comparison")
    print("=" * 70)
    
    test_messages = [
        {"role": "user", "content": "What are the benefits of cloud computing?"}
    ]
    
    patterns = [
        ("Sequential Processing", sequential_llm_call),
        ("Concurrent Processing", concurrent_llm_call)
    ]
    
    num_tests = 5
    
    for pattern_name, pattern_func in patterns:
        print(f"\nüß™ Testing {pattern_name}:")
        print("-" * 40)
        
        times = []
        success_count = 0
        
        for i in range(num_tests):
            start_time = time.time()
            try:
                result = await pattern_func(test_messages)
                end_time = time.time()
                duration = end_time - start_time
                times.append(duration)
                success_count += 1
                
                # Check if it's a GuardedResponse or direct response
                if hasattr(result, 'timing') and result.timing:
                    internal_timing = result.timing.get('total_time', duration)
                    print(f"  Test {i+1}: {duration:.3f}s (internal: {internal_timing:.3f}s)")
                else:
                    print(f"  Test {i+1}: {duration:.3f}s")
                    
            except Exception as e:
                print(f"  Test {i+1}: Failed - {e}")
        
        # Calculate statistics
        if times:
            avg_time = statistics.mean(times)
            min_time = min(times)
            max_time = max(times)
            std_dev = statistics.stdev(times) if len(times) > 1 else 0
            
            print(f"\nüìä {pattern_name} Results:")
            print(f"   ‚Ä¢ Average time: {avg_time:.3f}s")
            print(f"   ‚Ä¢ Min time: {min_time:.3f}s")
            print(f"   ‚Ä¢ Max time: {max_time:.3f}s")
            print(f"   ‚Ä¢ Std deviation: {std_dev:.3f}s")
            print(f"   ‚Ä¢ Success rate: {success_count}/{num_tests}")
        
        print()

async def benchmark_streaming_patterns():
    """Compare different streaming configurations"""
    print("üåä Streaming Pattern Performance Comparison")
    print("=" * 70)
    
    test_messages = [
        {"role": "user", "content": "Explain machine learning concepts"}
    ]
    
    streaming_patterns = [
        ("Fast Streaming (frequent checks)", fast_streaming_call),
        ("Balanced Streaming (moderate checks)", balanced_streaming_call)
    ]
    
    for pattern_name, pattern_func in streaming_patterns:
        print(f"\nüß™ Testing {pattern_name}:")
        print("-" * 40)
        
        start_time = time.time()
        chunk_count = 0
        total_content = ""
        
        try:
            async for event in pattern_func(test_messages):
                chunk_count += 1
                
                if isinstance(event, dict):
                    if event.get('type') == 'chunk':
                        content = event.get('content', '')
                        total_content += content
                        print(content, end='', flush=True)
                    elif event.get('type') == 'completed':
                        end_time = time.time()
                        duration = end_time - start_time
                        print(f"\n\nüìä Streaming completed:")
                        print(f"   ‚Ä¢ Total time: {duration:.3f}s")
                        print(f"   ‚Ä¢ Chunks processed: {chunk_count}")
                        print(f"   ‚Ä¢ Content length: {len(total_content)} chars")
                        print(f"   ‚Ä¢ Chars per second: {len(total_content)/duration:.1f}")
                        break
                    elif event.get('type') == 'error':
                        print(f"\n‚ùå Streaming error: {event.get('error')}")
                        break
                else:
                    # Handle raw chunks (fallback)
                    if hasattr(event, 'choices') and event.choices:
                        delta = event.choices[0].delta
                        if hasattr(delta, 'content') and delta.content:
                            total_content += delta.content
                            print(delta.content, end='', flush=True)
                            
        except Exception as e:
            print(f"\n‚ùå Streaming error: {e}")
        
        print("\n" + "=" * 40)

async def demonstrate_optimization_techniques():
    """Show various optimization techniques"""
    print("\nüöÄ Optimization Techniques Demo")
    print("=" * 70)
    
    print("1Ô∏è‚É£  Timeout Optimization:")
    print("   ‚Ä¢ Use shorter timeouts for faster failure detection")
    print("   ‚Ä¢ Balance between safety and responsiveness")
    print()
    
    # Example: Aggressive timeout for fast applications
    @guarded_chat_completion(
        agent_id=HALIOS_AGENT_ID,
        concurrent_guardrail_processing=True,
        guardrail_timeout=1.0  # Fast timeout
    )
    async def fast_timeout_call(messages):
        return await mock_llm_call(messages, delay=0.1)
    
    print("2Ô∏è‚É£  Stream Buffer Optimization:")
    print("   ‚Ä¢ Larger buffers = fewer guardrail checks = better performance")
    print("   ‚Ä¢ Smaller buffers = more checks = better safety")
    print()
    
    print("3Ô∏è‚É£  Concurrent Processing Benefits:")
    print("   ‚Ä¢ Guardrails run in parallel with LLM calls")
    print("   ‚Ä¢ Can save 50-80% of guardrail overhead")
    print("   ‚Ä¢ Recommended for production applications")
    print()
    
    print("4Ô∏è‚É£  Direct Client Usage for Advanced Control:")
    
    # Example: Advanced usage with direct client control
    async with ParallelGuardedChat(agent_id=HALIOS_AGENT_ID) as guard_client:
        start_time = time.time()
        
        result = await guard_client.guarded_call_parallel(
            messages=[{"role": "user", "content": "Hello optimization!"}],
            llm_func=lambda msgs: mock_llm_call(msgs, delay=0.2)
        )
        
        duration = time.time() - start_time
        
        print(f"   ‚Ä¢ Direct client call completed in {duration:.3f}s")
        print(f"   ‚Ä¢ Result type: {type(result).__name__}")
        if hasattr(result, 'timing'):
            print(f"   ‚Ä¢ Internal timing: {result.timing}")

async def performance_best_practices():
    """Demonstrate performance best practices"""
    print("\nüìã Performance Best Practices")
    print("=" * 70)
    
    practices = [
        {
            "title": "Use Concurrent Processing in Production",
            "description": "Set concurrent_guardrail_processing=True (default) for optimal performance",
            "code": "@guarded_chat_completion(agent_id='...', concurrent_guardrail_processing=True)"
        },
        {
            "title": "Optimize Timeout Values",
            "description": "Set guardrail_timeout based on your application's needs",
            "code": "@guarded_chat_completion(agent_id='...', guardrail_timeout=2.0)"
        },
        {
            "title": "Configure Streaming Appropriately", 
            "description": "Balance buffer size and check interval for your use case",
            "code": "@guarded_chat_completion(agent_id='...', streaming_guardrails=True, stream_buffer_size=100)"
        },
        {
            "title": "Use Sequential Only for Debugging",
            "description": "Sequential processing is slower but easier to debug",
            "code": "@guarded_chat_completion(agent_id='...', concurrent_guardrail_processing=False)"
        },
        {
            "title": "Monitor Performance Metrics",
            "description": "Use timing information to optimize your specific use case",
            "code": "result.timing['total_time']  # Check execution time"
        }
    ]
    
    for i, practice in enumerate(practices, 1):
        print(f"{i}Ô∏è‚É£  {practice['title']}:")
        print(f"   {practice['description']}")
        print(f"   Example: {practice['code']}")
        print()

async def main():
    """Run all performance examples"""
    await benchmark_processing_patterns()
    await benchmark_streaming_patterns()
    await demonstrate_optimization_techniques()
    await performance_best_practices()
    
    print("‚ú® Performance patterns example completed!")
    print("\nüí° Key Takeaways:")
    print("‚Ä¢ Concurrent processing provides significant performance benefits")
    print("‚Ä¢ Streaming configurations should match your safety/performance needs")
    print("‚Ä¢ Timeout values should be optimized for your specific use case")
    print("‚Ä¢ Sequential processing is useful for debugging but slower")
    print("‚Ä¢ Monitor timing metrics to optimize your implementation")

if __name__ == "__main__":
    import os
    
    # Set up demo environment if real credentials not provided
    if not os.getenv("HALIOS_API_KEY"):
        os.environ["HALIOS_API_KEY"] = "demo-key"
        os.environ["HALIOS_BASE_URL"] = "https://httpbin.org/status/200"
        print("üß™ Demo mode: Using mock endpoints (guardrail errors expected)")
        print("   Set HALIOS_API_KEY for real usage\n")
    
    asyncio.run(main())
