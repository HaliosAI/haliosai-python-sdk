#!/usr/bin/env python3
"""
Basic usage examples for HaliosAI SDK

This example demonstrates the core functionality of the HaliosAI SDK
using the new unified guarded_chat_completion decorator.
"""

import asyncio
import os
from typing import List, Dict, Any

# Import the unified decorator
from haliosai import guarded_chat_completion

# Mock LLM functions for demonstration
async def mock_openai_call(messages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Mock OpenAI-style API call"""
    await asyncio.sleep(0.1)  # Simulate API latency
    return {
        "choices": [{
            "message": {
                "role": "assistant",
                "content": f"Mock response to: {messages[-1].get('content', 'No content')}"
            }
        }],
        "usage": {"total_tokens": 50}
    }

async def mock_streaming_call(messages: List[Dict[str, Any]]):
    """Mock streaming API call"""
    response_chunks = ["Hello", " from", " HaliosAI", "! ", "Streaming", " works", " great", "."]
    
    for chunk in response_chunks:
        await asyncio.sleep(0.05)  # Simulate streaming delay
        yield {
            "choices": [{
                "delta": {"content": chunk}
            }]
        }

# Example 1: Basic usage with concurrent processing (recommended)
@guarded_chat_completion(agent_id="demo-basic")
async def basic_llm_call(messages: List[Dict[str, Any]]):
    """Basic LLM call with concurrent guardrail processing"""
    return await mock_openai_call(messages)

# Example 2: Sequential processing (useful for debugging)
@guarded_chat_completion(
    app_id="demo-sequential",
    concurrent_guardrail_processing=False
)
async def sequential_llm_call(messages: List[Dict[str, Any]]):
    """LLM call with sequential guardrail processing (easier to debug)"""
    return await mock_openai_call(messages)

# Example 3: Streaming with real-time guardrails
@guarded_chat_completion(
    app_id="demo-streaming",
    streaming_guardrails=True,
    stream_buffer_size=20,  # Check guardrails every 20 characters
    stream_check_interval=0.3  # Check every 300ms
)
async def streaming_llm_call(messages: List[Dict[str, Any]]):
    """Streaming LLM call with real-time guardrail evaluation"""
    async for chunk in mock_streaming_call(messages):
        yield chunk

# Example 4: Full configuration
@guarded_chat_completion(
    app_id="demo-full-config",
    api_key=os.getenv("HALIOS_API_KEY", "demo-key"),
    base_url=os.getenv("HALIOS_BASE_URL", "https://api.halioslabs.com"),
    concurrent_guardrail_processing=True,
    streaming_guardrails=False,
    stream_buffer_size=50,
    stream_check_interval=0.5,
    guardrail_timeout=5.0
)
async def fully_configured_llm_call(messages: List[Dict[str, Any]]):
    """Fully configured LLM call with all options specified"""
    return await mock_openai_call(messages)

async def main():
    """Run all examples"""
    print("üöÄ HaliosAI Basic Usage Examples")
    print("=" * 50)
    
    # Test messages
    test_messages = [
        {"role": "user", "content": "Hello! How are you today?"}
    ]
    
    # Example 1: Basic concurrent processing
    print("\n1Ô∏è‚É£  Basic LLM call (concurrent guardrail processing)")
    try:
        result = await basic_llm_call(test_messages)
        print(f"‚úÖ Success: {result}")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Example 2: Sequential processing
    print("\n2Ô∏è‚É£  Sequential processing (debugging mode)")
    try:
        result = await sequential_llm_call(test_messages)
        print(f"‚úÖ Success: {result}")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Example 3: Streaming
    print("\n3Ô∏è‚É£  Streaming with real-time guardrails")
    try:
        print("üì° Streaming output: ", end="")
        async for event in streaming_llm_call(test_messages):
            if event.get('type') == 'chunk':
                print(event.get('content', ''), end='')
            elif event.get('type') == 'completed':
                print("\n‚úÖ Stream completed successfully!")
            elif event.get('type') == 'error':
                print(f"\n‚ùå Stream error: {event.get('error')}")
                break
        else:
            # If loop completed without break
            print("\n‚úÖ Streaming completed")
    except Exception as e:
        print(f"\n‚ùå Streaming error: {e}")
    
    # Example 4: Full configuration
    print("\n4Ô∏è‚É£  Fully configured LLM call")
    try:
        result = await fully_configured_llm_call(test_messages)
        print(f"‚úÖ Success: {result}")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    print("\n" + "=" * 50)
    print("‚ú® All examples completed!")
    
    print("\nüí° Notes:")
    print("- Guardrail errors are expected in demo mode")
    print("- Set HALIOS_API_KEY environment variable for real usage")
    print("- concurrent_guardrail_processing=True is recommended for production")
    print("- Use concurrent_guardrail_processing=False for debugging")

if __name__ == "__main__":
    # Set up environment for demo
    os.environ.setdefault("HALIOS_API_KEY", "demo-key")
    os.environ.setdefault("HALIOS_BASE_URL", "https://httpbin.org/status/200")  # Mock endpoint
    
    asyncio.run(main())
    
    try:
        messages = [{"role": "user", "content": "Process this quickly!"}]
        result = await fast_llm_call(messages)
        
        if hasattr(result, 'final_response'):
            print(f"Final response: {result.final_response}")
            print(f"Timing info: {result.timing}")
        else:
            print(f"Response: {result}")
    except Exception as e:
        print(f"Error: {e}")


async def streaming_example():
    """Example using streaming guardrails"""
    print("\\n=== Streaming Example ===")
    
    from haliosai import streaming_guarded_chat
    
    @streaming_guarded_chat(app_id="demo-app", stream_buffer_size=20)
    async def stream_llm_call(messages):
        """Streaming LLM call"""
        print(f"Streaming LLM called with {len(messages)} messages")
        
        # Simulate streaming chunks
        response_text = "This is a simulated streaming response from the LLM. "
        response_text += "Each chunk is processed with real-time guardrails!"
        
        for i in range(0, len(response_text), 10):
            chunk = response_text[i:i+10]
            yield {
                "choices": [
                    {"delta": {"content": chunk}}
                ]
            }
            await asyncio.sleep(0.05)  # Simulate streaming delay
    
    try:
        messages = [{"role": "user", "content": "Stream me a response!"}]
        
        async for event in stream_llm_call(messages):
            if event['type'] == 'chunk':
                print(event['content'], end='', flush=True)
            elif event['type'] == 'completed':
                print(f"\\nStreaming completed! Final response: {event['final_content'][:50]}...")
                break
            elif event['type'] == 'violation':
                print(f"\\nContent blocked: {event['violations']}")
                break
    except Exception as e:
        print(f"\\nStreaming error: {e}")


async def agents_integration_example():
    """Example using OpenAI Agents integration"""
    print("\\n=== OpenAI Agents Integration Example ===")
    
    try:
        # Single agent mode
        with patch_openai_agents(app_id="demo-app") as patcher:
            print("OpenAI Agents framework patched with HaliosAI guardrails")
            
            # Simulate agent workflow
            async def mock_agent_call():
                print("Simulating OpenAI agent call...")
                return "Agent completed task successfully!"
            
            result = await mock_agent_call()
            print(f"Agent result: {result}")
            
    except Exception as e:
        print(f"Agents integration error: {e}")


async def multi_agent_example():
    """Example using multi-agent configurations"""
    print("\\n=== Multi-Agent Configuration Example ===")
    
    from haliosai import patch_openai_agents_multi
    
    # Configure different guardrail profiles for different agents
    agent_config = {
        'orchestrator': {
            'app_id': 'demo-orchestrator', 
            'description': 'Main coordination agent'
        },
        'translator': {
            'app_id': 'demo-translation', 
            'description': 'Translation specialist'
        },
        'synthesizer': {
            'app_id': 'demo-synthesis', 
            'description': 'Content synthesis agent'
        }
    }
    
    try:
        with patch_openai_agents_multi(agent_config) as patcher:
            print("Multi-agent configuration applied")
            
            # Simulate multi-agent workflow
            async def mock_multi_agent_workflow():
                print("Simulating orchestrator agent...")
                await asyncio.sleep(0.1)
                
                print("Simulating translator agent...")
                await asyncio.sleep(0.1)
                
                print("Simulating synthesizer agent...")
                await asyncio.sleep(0.1)
                
                return "Multi-agent workflow completed!"
            
            result = await mock_multi_agent_workflow()
            print(f"Multi-agent result: {result}")
            
    except Exception as e:
        print(f"Multi-agent error: {e}")


def configuration_example():
    """Example showing configuration options"""
    print("\\n=== Configuration Example ===")
    
    # Method 1: Environment variables
    print("Method 1: Using environment variables")
    print("export HALIOS_API_KEY='your-api-key'")
    print("export HALIOS_BASE_URL='https://api.halioslabs.com'")
    print("export HALIOS_LOG_LEVEL='DEBUG'")
    
    # Method 2: Programmatic configuration
    print("\\nMethod 2: Programmatic configuration")
    from haliosai import HaliosGuard
    
    guard_instance = HaliosGuard(
        app_id="demo-app",
        api_key="your-api-key",
        base_url="https://api.halioslabs.com",
        parallel=True
    )
    print(f"Created guard with app_id: {guard_instance.app_id}")
    print(f"Parallel mode: {guard_instance.parallel}")
    
    # Method 3: Logging configuration
    print("\\nMethod 3: Logging configuration")
    from haliosai.config import setup_logging
    setup_logging("DEBUG")
    print("Logging configured to DEBUG level")


async def main():
    """Run all examples"""
    print("HaliosAI SDK Examples")
    print("=" * 50)
    
    # Show configuration first
    configuration_example()
    
    # Run async examples
    await basic_decorator_example()
    await parallel_processing_example()
    await streaming_example()
    await agents_integration_example()
    await multi_agent_example()
    
    print("\\n" + "=" * 50)
    print("All examples completed!")
    print("\\nNext steps:")
    print("1. Set your HALIOS_API_KEY environment variable")
    print("2. Configure your guardrail applications at https://halioslabs.com")
    print("3. Integrate HaliosAI with your LLM applications")


if __name__ == "__main__":
    # Set demo environment variables if not already set
    if not os.getenv("HALIOS_API_KEY"):
        os.environ["HALIOS_API_KEY"] = "demo-key"
    if not os.getenv("HALIOS_BASE_URL"):
        os.environ["HALIOS_BASE_URL"] = "http://localhost:2000"
    
    asyncio.run(main())
